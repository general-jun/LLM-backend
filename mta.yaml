_schema-version: 3.3.0
ID: llm-backend
description: Knowledge Management and Chat Service Using RAG and SAP AI Core
version: 1.0.0
parameters:
  enable-parallel-deployments: true
build-parameters:
  before-all:
  - builder: custom
    commands:
    - npm ci
    - npx cds build --production


########################################
# - Deploy Location 
#   Subaccount -> Space -> Applications
########################################
modules:
# OData servcie Application
- name: llm-backend-app
  type: nodejs
  path: gen/srv
  requires:
  - name: llm-xsuaa-app
  - name: llm-db-rc
  provides:
  - name: srv-api
    properties:
      srv-url: ${default-url}
  parameters:
    buildpack: nodejs_buildpack
    instances: 1
  build-parameters:
    builder: npm-ci
    ignore:
    - node_modules/
# Database Object Deploy Application
- name: llm-deployer-app
  type: hdb
  path: gen/db
  requires:
  - name: llm-db-rc
    group: SERVICE_REPLACEMENTS
    properties:
      key: ServiceName_1
      service: ~{the-service-name}


#############################################
# - Deploy Location
#   Subaccount -> Space -> Service Instances
#############################################
resources:
# Managed Approuter Application
- name: llm-xsuaa-app
  type: org.cloudfoundry.managed-service
  parameters:
    config:
      tenant-mode: dedicated
      xsappname: llm-auth-app
    path: ./xs-security.json
    service: xsuaa
    service-plan: application
# HDI Container Service
- name: llm-db-rc
  type: org.cloudfoundry.existing-service
  parameters:
    service-name: llm-db-svc
  properties:
    the-service-name: ${service-name}